请叫我刁奋进:
activation functions

请叫我刁奋进:
(1) sigmoid

请叫我刁奋进:
(2) tanh

请叫我刁奋进:
(3) relu

请叫我刁奋进:
y = f(x)

请叫我刁奋进:
sigmoid 和 tanh 的四个特点：（1）monotonic 单调性 -》 单调增；（2）能转化为概率；（3）一阶导好算；（4）训练符合一般认知

请叫我刁奋进:
第4点是这样的：x在接近0的时候，斜率最大，训练最快；x在远离0

请叫我刁奋进:
的时候，斜率接近于0，进步程度就很小

请叫我刁奋进:
x=0的分界正好就是类别的边界，x<0 是A，x>0 是B

请叫我刁奋进:
因为我们一般认知就是更在意x=0附近的训练，斜率大是有利的；两边的话，努力很多也只是把概率从95%提升到96%，大多数时候不太重要

请叫我刁奋进:
-----分割线------

请叫我刁奋进:
刚才是backward propogation更有效 还有一个原因 在从右往左算每一块东西的时候 很多时候这个值在从左往右算的时候已经算出来了

请叫我刁奋进:
比如sigmoid的一阶导是 a(1-a)；这也能进一步减少需要计算的东西
